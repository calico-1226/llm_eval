import time
from typing import Optional, List, Union, Tuple, Dict

import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

import bigbench.api.model as model
import bigbench.models.model_utils as model_utils

MODEL_INFO = {"google/flan-t5-small": None}


class FlanT5(model.Model):
    """Flan T5 model."""

    def __init__(self, model_name: str, max_length=256, show_progress=True) -> None:
        if model_name not in MODEL_INFO:
            raise ValueError(f"Model {model_name} not supported.")

        self._model_name = model_name
        self._max_length = max_length
        self._show_progress = show_progress
        self._model = AutoModelForSeq2SeqLM.from_pretrained(
            model_name, device_map="auto"
        )
        self._tokenizer = AutoTokenizer.from_pretrained(model_name)
        self._device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def generate_text(
        self,
        inputs: Union[str, List[str]],
        max_length: int = 0,
        stop_string: Optional[str] = None,
        output_regex: Optional[str] = None,
    ) -> Union[str, List[str]]:
        """Generates text for given inputs.

        Args:
          inputs: String or list of strings as inputs for model.
          max_length: Maximum string length of output, if 0 uses max_length passed
            to constructor
          stop_string: If specified, model output will be truncated to the shortest
            string which includes stop_string.
          output_regex: If specified, the first match to the python regular
            expression output_regex in the model output will be returned. If there is
            no match, an empty string will be returned.

        Returns:
          String or list of strings generated by model.

        Raises:
          ValueError if max_length is invalid
        """
        max_length = max_length or self._max_length

        if isinstance(inputs, str):
            input_list = [inputs]
        else:
            input_list = inputs

        generated = []
        last = start = time.time()

        for idx, input_text in enumerate(input_list):
            input = self._tokenizer(input_text, return_tensors="pt").to(self._device)
            output = self._model.generate(**input)
            output_text = self._tokenizer.decode(output[0], skip_special_tokens=True)
            generated.append(output_text)

            if self._show_progress and time.time() - last > 60:
                print(
                    f"generated {idx} of {len(input_list)} outputs in {int(time.time() - start)} secs..."
                )
                last = time.time()

        if isinstance(inputs, str):
            generated = generated[0]

        generated = model_utils.postprocess_output(
            generated, max_length, stop_string, output_regex
        )
        return generated
